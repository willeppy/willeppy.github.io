<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-05-17T00:21:44-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Will Epperson</title><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><entry><title type="html">Solas</title><link href="http://localhost:4000/papers/solas" rel="alternate" type="text/html" title="Solas" /><published>2022-04-24T00:00:00-04:00</published><updated>2022-04-24T00:00:00-04:00</updated><id>http://localhost:4000/papers/solas</id><content type="html" xml:base="http://localhost:4000/papers/solas">&lt;h1 id=&quot;leveraging-analysis-history-for-improved-in-situ-visualization-recommendation&quot;&gt;Leveraging Analysis History for Improved In Situ Visualization Recommendation&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://dorisjunglinlee.com&quot;&gt;Doris Jung-Lin Lee&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://leijiewang.com&quot;&gt;Leijie Wang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.kunal-agarwal.com&quot;&gt;Kunal Agarwal&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://people.eecs.berkeley.edu/~adityagp/&quot;&gt;Aditya Parameswaran&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/solas-eurovis22.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      Solas tracks the history of a user’s analysis to provide improved in situ visualization recommendations. Above, a user has most
recently created the Class column that is visualized on the left side of the interface. Recently executed Pandas commands interacted with
Worldwide_Gross, Viewership, and MPAA_Rating; therefore, Class is shown relative to these columns.


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Existing visualization recommendation systems commonly rely on a single snapshot of a dataset to suggest visualizations to users. 
However, exploratory data analysis involves a series of related interactions with a dataset over time rather than one-off analytical steps. 
We present Solas, a tool that tracks the history of a user’s data analysis, models their interest in each column, and 
uses this information to provide visualization recommendations, all within the user’s native analytical environment. 
Recommending with analysis history improves visualizations in three primary ways: task-specific visualizations use the provenance of 
data to provide sensible encodings for common analysis functions, aggregated history is used to rank visualizations by our model of a 
user’s interest in each column, and column data types are inferred based on applied operations. We present a usage scenario and a user evaluation 
demonstrating how leveraging analysis history improves in situ visualization recommendations on real-world analysis tasks.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Leveraging Analysis History for Improved In Situ Visualization Recommendation&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://dorisjunglinlee.com&quot;&gt;Doris Jung-Lin Lee&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://leijiewang.com&quot;&gt;Leijie Wang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.kunal-agarwal.com&quot;&gt;Kunal Agarwal&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://people.eecs.berkeley.edu/~adityagp/&quot;&gt;Aditya Parameswaran&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Solas is a visualization recommendation tool that uses the history of analysis for in situ recommendations in Jupyter.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;EuroVis 22: Eurographics Conference on Visualization (EuroVis). Rome, Italy, 2022.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/solas&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/Solas_EuroVis22.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	
	  &lt;a href=&quot;https://github.com/cmudig/solas&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{Epperson22Solas,
  title={Leveraging Analysis History for Improved In Situ Visualization Recommendation},
  author={Epperson, Will and Lee, Doris Jung-Lin and Wang, Leijie and Agarwal, Kunal and Parameswaran, Aditya and Moritz, Dominik and Perer, Adam},
  journal={EuroVis 22: Eurographics Conference on Visualization},
  year={2022},
  publisher={EG}
  url={}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Leveraging Analysis History for Improved In Situ Visualization Recommendation Will Epperson, Doris Jung-Lin Lee, Leijie Wang, Kunal Agarwal, Aditya Parameswaran, Dominik Moritz, Adam Perer Solas tracks the history of a user’s analysis to provide improved in situ visualization recommendations. Above, a user has most recently created the Class column that is visualized on the left side of the interface. Recently executed Pandas commands interacted with Worldwide_Gross, Viewership, and MPAA_Rating; therefore, Class is shown relative to these columns. Abstract Existing visualization recommendation systems commonly rely on a single snapshot of a dataset to suggest visualizations to users. However, exploratory data analysis involves a series of related interactions with a dataset over time rather than one-off analytical steps. We present Solas, a tool that tracks the history of a user’s data analysis, models their interest in each column, and uses this information to provide visualization recommendations, all within the user’s native analytical environment. Recommending with analysis history improves visualizations in three primary ways: task-specific visualizations use the provenance of data to provide sensible encodings for common analysis functions, aggregated history is used to rank visualizations by our model of a user’s interest in each column, and column data types are inferred based on applied operations. We present a usage scenario and a user evaluation demonstrating how leveraging analysis history improves in situ visualization recommendations on real-world analysis tasks. Citation Leveraging Analysis History for Improved In Situ Visualization Recommendation Will Epperson, Doris Jung-Lin Lee, Leijie Wang, Kunal Agarwal, Aditya Parameswaran, Dominik Moritz, Adam Perer Solas is a visualization recommendation tool that uses the history of analysis for in situ recommendations in Jupyter. EuroVis 22: Eurographics Conference on Visualization (EuroVis). Rome, Italy, 2022. Project PDF Code</summary></entry><entry><title type="html">Ditl</title><link href="http://localhost:4000/papers/ditl" rel="alternate" type="text/html" title="Ditl" /><published>2022-02-16T00:00:00-05:00</published><updated>2022-02-16T00:00:00-05:00</updated><id>http://localhost:4000/papers/ditl</id><content type="html" xml:base="http://localhost:4000/papers/ditl">&lt;h1 id=&quot;diff-in-the-loop-supporting-data-comparison-in-exploratory-data-analysis&quot;&gt;Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/ditl-chi22.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      As users iterate on their data during analysis, they can use DITL to compare data snapshots. 
Every time users successfully execute code we save a snapshot (A). 
Users can compare the code using traditional code diffing tools. 
Additionally, users can also use DITL to compare data iterations with interactive visualizations, descriptive statistics, and data preview (B). 
Users can choose three ways to visualize the differences in each column: the delta view (C), opacity view (D), and parallel view (E).


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Data science is characterized by evolution: since data science is exploratory, results evolve from moment to moment; since it can be collaborative, results evolve as the work changes hands. 
While existing tools help data scientists track changes in code, they provide less support for understanding the iterative changes that the code produces in the data. 
We explore the idea of visualizing differences in datasets as a core feature of exploratory data analysis, a concept we call Diff in the Loop (DITL). 
We evaluated DITL in a user study with 16 professional data scientists and found it helped them understand the implications of their actions when manipulating data. 
We summarize these findings and discuss how the approach can be generalized to different data science workflows.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Diff in the Loop supports tracking, comparing, and visualizing differences in datasets during iterative data analysis.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;SIGCHI 22: ACM Symposium on Computer Human Interaction (CHI). New Orleans, LA, 2022.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/ditl&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/ditl-chi22.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{wang2022DITL,
  title={Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis},
  author={Wang, April Yi and Epperson, Will, and DeLine, Robert and Drucker, Steven M.},
  journal={SIGCHI 22: ACM Symposium on Computer Human Interaction},
  year={2022},
  publisher={ACM}
  url={}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis April Yi Wang, Will Epperson, Robert DeLine, Steven M. Drucker As users iterate on their data during analysis, they can use DITL to compare data snapshots. Every time users successfully execute code we save a snapshot (A). Users can compare the code using traditional code diffing tools. Additionally, users can also use DITL to compare data iterations with interactive visualizations, descriptive statistics, and data preview (B). Users can choose three ways to visualize the differences in each column: the delta view (C), opacity view (D), and parallel view (E). Abstract Data science is characterized by evolution: since data science is exploratory, results evolve from moment to moment; since it can be collaborative, results evolve as the work changes hands. While existing tools help data scientists track changes in code, they provide less support for understanding the iterative changes that the code produces in the data. We explore the idea of visualizing differences in datasets as a core feature of exploratory data analysis, a concept we call Diff in the Loop (DITL). We evaluated DITL in a user study with 16 professional data scientists and found it helped them understand the implications of their actions when manipulating data. We summarize these findings and discuss how the approach can be generalized to different data science workflows. Citation Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis April Yi Wang, Will Epperson, Robert DeLine, Steven M. Drucker Diff in the Loop supports tracking, comparing, and visualizing differences in datasets during iterative data analysis. SIGCHI 22: ACM Symposium on Computer Human Interaction (CHI). New Orleans, LA, 2022. Project PDF</summary></entry><entry><title type="html">Reuseshareds</title><link href="http://localhost:4000/papers/reuse-ds" rel="alternate" type="text/html" title="Reuseshareds" /><published>2022-02-16T00:00:00-05:00</published><updated>2022-02-16T00:00:00-05:00</updated><id>http://localhost:4000/papers/reuseshareds</id><content type="html" xml:base="http://localhost:4000/papers/reuse-ds">&lt;h1 id=&quot;strategies-for-reuse-and-sharing-among-data-scientists-in-software-teams&quot;&gt;Strategies for Reuse and Sharing among Data Scientists in Software Teams&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/reuse-share-DS-icse22.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      Five unique strategies are used to reuse and share analysis code in data science. Personal reuse strategies like reusing one&apos;s own code are common, whereas using template notebooks is more rare and dependent on tool support.


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Effective sharing and reuse practices have long been hallmarks of proficient software engineering. 
Yet the exploratory nature of data science presents new challenges and opportunities to support sharing and reuse of analysis code. 
To better understand current practices, we conducted interviews (N=17) and a survey (N=132) with data scientists at Microsoft, and extract five commonly used strategies for sharing and reuse of past work: 
personal analysis reuse, personal utility libraries, team shared analysis code, team shared template notebooks, and team shared libraries. 
We also identify factors that encourage or discourage data scientists from sharing and reusing. 
Our participants described obstacles to reuse and sharing including a lack of incentives to create shared code, difficulties in making data science code modular, and a lack of tool interoperability. 
We discuss how future tools might help meet these needs.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Strategies for Reuse and Sharing among Data Scientists in Software Teams&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Interviews and a survey with 149 data scientists at Microsoft revealed five distinct strategies for sharing and reusing analysis code along with factors that encourage or discourage reuse.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;ICSE 22: ACM International Conference on Software Engineering (ICSE). Pittsburgh, PA, 2022.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/reuse-ds&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/reuse-sharing-DS-icse22.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	
	&lt;a href=&quot;https://www.youtube.com/watch?v=W4XAF2vkoCQ&quot;&gt;
		&lt;i class=&quot;fas fa-video&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Recording
	&lt;/a&gt;
	

	

	

	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{wang2022DITL,
  title={Strategies for Reuse and Sharing among Data Scientists in Software Teams},
  author={Epperson, Will and Wang, April Yi and DeLine, Robert and Drucker, Steven M.},
  journal={ICSE 22: ACM International Conference on Software Engineering},
  year={2022},
  publisher={ACM}
  url={}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Strategies for Reuse and Sharing among Data Scientists in Software Teams Will Epperson, April Yi Wang, Robert DeLine, Steven M. Drucker Five unique strategies are used to reuse and share analysis code in data science. Personal reuse strategies like reusing one&apos;s own code are common, whereas using template notebooks is more rare and dependent on tool support. Abstract Effective sharing and reuse practices have long been hallmarks of proficient software engineering. Yet the exploratory nature of data science presents new challenges and opportunities to support sharing and reuse of analysis code. To better understand current practices, we conducted interviews (N=17) and a survey (N=132) with data scientists at Microsoft, and extract five commonly used strategies for sharing and reuse of past work: personal analysis reuse, personal utility libraries, team shared analysis code, team shared template notebooks, and team shared libraries. We also identify factors that encourage or discourage data scientists from sharing and reusing. Our participants described obstacles to reuse and sharing including a lack of incentives to create shared code, difficulties in making data science code modular, and a lack of tool interoperability. We discuss how future tools might help meet these needs. Citation Strategies for Reuse and Sharing among Data Scientists in Software Teams Will Epperson, April Yi Wang, Robert DeLine, Steven M. Drucker Interviews and a survey with 149 data scientists at Microsoft revealed five distinct strategies for sharing and reusing analysis code along with factors that encourage or discourage reuse. ICSE 22: ACM International Conference on Software Engineering (ICSE). Pittsburgh, PA, 2022. Project PDF Recording</summary></entry><entry><title type="html">Recast</title><link href="http://localhost:4000/papers/recast" rel="alternate" type="text/html" title="Recast" /><published>2020-01-08T00:00:00-05:00</published><updated>2020-01-08T00:00:00-05:00</updated><id>http://localhost:4000/papers/recast</id><content type="html" xml:base="http://localhost:4000/papers/recast">&lt;h1 id=&quot;recast-interactive-auditing-of-automatic-toxicity-detection-models&quot;&gt;RECAST: Interactive Auditing of Automatic Toxicity Detection Models&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://austinpwright.com&quot;&gt;Austin P. Wright&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://oshaikh.com/&quot;&gt;Omar Shaikh&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://haekyu.com&quot;&gt;Haekyu Park&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
Muhammed Ahmed,
  

  
    
Stephane Pinel,
  

  
    
&lt;a href=&quot;https://www.cc.gatech.edu/~dyang888/&quot;&gt;Diyi Yang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.cc.gatech.edu/~dchau&quot;&gt;Duen Horng (Polo) Chau&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/20-recast-chi.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      A: RECAST consists of a textbox and a radial progress bar. A color change on the radial progress, along with a score, indicate the toxicity of a sentence. 
Toxicity ranges from white (non-toxic) to red (very toxic). Users can hover over options to preview toxicity scores for replacing the selected word in the sentence. 
B: upon replacing the word (in the case of this figure, replacing “idiotic” with “nonsensical”), the main radial progress bar reflects the reduced toxicity score. 
However the small attention on the other pejorative word &quot;moron&quot; compared to &quot;video&quot; in the alternative version shows the idiosyncrasies of the model and underlying dataset.


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As toxic language becomes nearly pervasive online, there has been increasing interest in leveraging the advancements in natural language processing (NLP) to automatically detect and remove toxic comments. 
Despite fairness concerns and limited interpretability, there is currently little work for auditing these systems in particular for end users. 
We present our ongoing work, RECAST, an interactive tool for auditing toxicity detection models by visualizing explanations for predictions and providing alternative wordings for detected toxic speech. 
RECAST displays the attention of toxicity detection models on user input, and provides an intuitive system for rewording impactful language within a comment with less toxic alternative words close in embedding space. 
Finally we propose a larger user study  of RECAST, with promising preliminary results, to validate it’s effectiveness and useability with end users.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;RECAST: Interactive Auditing of Automatic Toxicity Detection Models&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://austinpwright.com&quot;&gt;Austin P. Wright&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://oshaikh.com/&quot;&gt;Omar Shaikh&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://haekyu.com&quot;&gt;Haekyu Park&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
Muhammed Ahmed,
	

	
		
Stephane Pinel,
	

	
		
&lt;a href=&quot;https://www.cc.gatech.edu/~dyang888/&quot;&gt;Diyi Yang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.cc.gatech.edu/~dchau&quot;&gt;Duen Horng (Polo) Chau&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Interactive Auditing of Automatic Toxicity Detection Models
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;24th ACM Conference on Computer-Supported Cooperative Work &amp;amp; Social Computing.  2021.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/recast&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;https://arxiv.org/pdf/2102.04427&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{wright2020recast,
title={RECAST: Interactive Auditing of Automatic Toxicity Detection Models},
author={Austin P. Wright and Omar Shaikh and Haekyu Park and Will Epperson and Muhammed Ahmed and Stephane Pinel and Diyi Yang and Duen Horng (Polo) Chau},
year={2020},
eprint={2001.01819},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">RECAST: Interactive Auditing of Automatic Toxicity Detection Models Austin P. Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed, Stephane Pinel, Diyi Yang, Duen Horng (Polo) Chau A: RECAST consists of a textbox and a radial progress bar. A color change on the radial progress, along with a score, indicate the toxicity of a sentence. Toxicity ranges from white (non-toxic) to red (very toxic). Users can hover over options to preview toxicity scores for replacing the selected word in the sentence. B: upon replacing the word (in the case of this figure, replacing “idiotic” with “nonsensical”), the main radial progress bar reflects the reduced toxicity score. However the small attention on the other pejorative word &quot;moron&quot; compared to &quot;video&quot; in the alternative version shows the idiosyncrasies of the model and underlying dataset. Abstract As toxic language becomes nearly pervasive online, there has been increasing interest in leveraging the advancements in natural language processing (NLP) to automatically detect and remove toxic comments. Despite fairness concerns and limited interpretability, there is currently little work for auditing these systems in particular for end users. We present our ongoing work, RECAST, an interactive tool for auditing toxicity detection models by visualizing explanations for predictions and providing alternative wordings for detected toxic speech. RECAST displays the attention of toxicity detection models on user input, and provides an intuitive system for rewording impactful language within a comment with less toxic alternative words close in embedding space. Finally we propose a larger user study of RECAST, with promising preliminary results, to validate it’s effectiveness and useability with end users. Citation RECAST: Interactive Auditing of Automatic Toxicity Detection Models Austin P. Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed, Stephane Pinel, Diyi Yang, Duen Horng (Polo) Chau Interactive Auditing of Automatic Toxicity Detection Models 24th ACM Conference on Computer-Supported Cooperative Work &amp;amp; Social Computing. 2021. Project PDF</summary></entry><entry><title type="html">Fairvis</title><link href="http://localhost:4000/papers/fairvis" rel="alternate" type="text/html" title="Fairvis" /><published>2019-10-15T00:00:00-04:00</published><updated>2019-10-15T00:00:00-04:00</updated><id>http://localhost:4000/papers/fairvis</id><content type="html" xml:base="http://localhost:4000/papers/fairvis">&lt;h1 id=&quot;fairvis-visual-analytics-for-discovering-intersectional-bias-in-machine-learning&quot;&gt;FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://cabreraalex.com&quot;&gt;Angel Cabrera&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://fredhohman.com&quot;&gt;Fred Hohman&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://minsuk.com&quot;&gt;Minsuk Kahng&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://jamiemorgenstern.com&quot;&gt;Jamie Morgenstern&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.cc.gatech.edu/~dchau&quot;&gt;Duen Horng (Polo) Chau&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/19-fairvis-vast.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      FairVis integrates multiple coordinated views for discovering intersectional bias. 
Above, our user investigates the intersectional subgroups of &lt;i&gt;sex&lt;/i&gt; and &lt;i&gt;race&lt;/i&gt;. 
A. The Feature Distribution View allows users to visualize each feature&apos;s distribution and generate subgroups.
B. The Subgroup Overview lets users select various fairness metrics to see the global average per metric and compare subgroups to one another, e.g., pinned Caucasian Males versus hovered African-American Males.
The plots for &lt;i&gt;Recall&lt;/i&gt; and &lt;i&gt;False Positive Rate&lt;/i&gt; show that for African-American Males, the model has relatively high recall but also the highest false positive rate out of all subgroups of sex and race. 
C. The Detailed Comparison View lets users compare the details of two groups and investigate their class balances.
Since the difference in False Positive Rates between Caucasian Males and African-American Males is far larger than their difference in base rates, a user suspects this part of the model merits further inquiry. 
D. The Suggested and Similar Subgroup View shows suggested subgroups ranked by the worst performance in a given metric. 


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people.
Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups.
Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups.
We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models.
Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups.
FairVis’ coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups.
We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism.
As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://cabreraalex.com&quot;&gt;Angel Cabrera&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://fredhohman.com&quot;&gt;Fred Hohman&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://minsuk.com&quot;&gt;Minsuk Kahng&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://jamiemorgenstern.com&quot;&gt;Jamie Morgenstern&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.cc.gatech.edu/~dchau&quot;&gt;Duen Horng (Polo) Chau&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Discovering intersectional ML Bias through interactive visualization.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;IEEE Conference on Visual Analytics Science and Technology (VAST). Vancouver, Canada, 2019.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/fairvis&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	
	&lt;a href=&quot;https://poloclub.github.io/FairVis/&quot;&gt;
		&lt;i class=&quot;fas fa-play&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Demo
	&lt;/a&gt;
	

	
	  &lt;a href=&quot;https://arxiv.org/abs/1904.05419&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	
	  &lt;a href=&quot;https://medium.com/@cabreraalex/fairvis-discovering-bias-in-machine-learning-using-visual-analytics-acbd362a3e2f&quot;&gt;
	    &lt;i class=&quot;fas fa-newspaper&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Blog
	  &lt;/a&gt;
	

	

	

	

	
	&lt;a href=&quot;https://vimeo.com/368702211&quot;&gt;
		&lt;i class=&quot;fas fa-video&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Recording
	&lt;/a&gt;
	

	
	  &lt;a href=&quot;/slides/19-fairvis-vast-slides.pdf&quot;&gt;
	    &lt;i class=&quot;fas fa-window-maximize&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Slides
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;https://github.com/poloclub/FairVis&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```

@article{cabrera2019fairvis,
  title={FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning},
  author={Cabrera, {\&apos;A}ngel and Epperson, Will, and Hohman, Fred and Kahng, Minsuk and Morgenstern, Jamie and Chau, Duen Horng},
  journal={IEEE Conference on Visual Analytics Science and Technology (VAST)},
  year={2019},
  publisher={IEEE}
  url={https://poloclub.github.io/FairVis/}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning Angel Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, Duen Horng (Polo) Chau FairVis integrates multiple coordinated views for discovering intersectional bias. Above, our user investigates the intersectional subgroups of sex and race. A. The Feature Distribution View allows users to visualize each feature&apos;s distribution and generate subgroups. B. The Subgroup Overview lets users select various fairness metrics to see the global average per metric and compare subgroups to one another, e.g., pinned Caucasian Males versus hovered African-American Males. The plots for Recall and False Positive Rate show that for African-American Males, the model has relatively high recall but also the highest false positive rate out of all subgroups of sex and race. C. The Detailed Comparison View lets users compare the details of two groups and investigate their class balances. Since the difference in False Positive Rates between Caucasian Males and African-American Males is far larger than their difference in base rates, a user suspects this part of the model merits further inquiry. D. The Suggested and Similar Subgroup View shows suggested subgroups ranked by the worst performance in a given metric. Abstract The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis’ coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems. Citation FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning Angel Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, Duen Horng (Polo) Chau Discovering intersectional ML Bias through interactive visualization. IEEE Conference on Visual Analytics Science and Technology (VAST). Vancouver, Canada, 2019. Project Demo PDF Blog Recording Slides Code</summary></entry></feed>