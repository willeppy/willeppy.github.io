<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-20T18:13:39-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Will Epperson</title><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><entry><title type="html">Overreliance</title><link href="http://localhost:4000/papers/overreliance" rel="alternate" type="text/html" title="Overreliance" /><published>2025-05-13T00:00:00-04:00</published><updated>2025-05-13T00:00:00-04:00</updated><id>http://localhost:4000/papers/overreliance</id><content type="html" xml:base="http://localhost:4000/papers/overreliance">&lt;h1 id=&quot;over-relying-on-reliance-towards-realistic-evaluations-of-ai-based-clinical-decision-support&quot;&gt;Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;https://venkatesh-sivaraman.github.io/&quot;&gt;Venkatesh Sivaraman&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://katelyn98.github.io/&quot;&gt;Katelyn Morrison&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/25-overreliance.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      We discuss how evaluations of human-AI decision support systems can move beyond reliance as the primary metric.

    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As AI-based clinical decision support (AI-CDS) is introduced in more and more aspects of healthcare services, HCI research plays an increasingly important role in designing for complementarity between AI and clinicians. However, current evaluations of AI-CDS often fail to capture when AI is and is not useful to clinicians. This position paper reflects on our work and influential AI-CDS literature to advocate for moving beyond evaluation metrics like Trust, Reliance, Acceptance, and Performance on the AI’s task (what we term the “trap” of human-AI collaboration). Although these metrics can be meaningful in some simple scenarios, we argue that optimizing for them ignores important ways that AI falls short of clinical benefit, as well as ways that clinicians successfully use AI. As the fields of HCI and AI in healthcare develop new ways to design and evaluate CDS tools, we call on the community to prioritize ecologically valid, domain-appropriate study setups that measure the emergent forms of value that AI can bring to healthcare professionals.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;https://venkatesh-sivaraman.github.io/&quot;&gt;Venkatesh Sivaraman&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://katelyn98.github.io/&quot;&gt;Katelyn Morrison&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	We discuss how evaluations of human-AI decision support systems can move beyond reliance as the primary metric.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;2025 CHI Workshop on HCI+Health (CHI). Yokohama, Japan, 2025.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/overreliance&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;https://arxiv.org/abs/2504.07423&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	

	

	

	

	

	
	
	&lt;/span&gt;




&lt;/p&gt;

&lt;!-- ## BibTeX
```


``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support Venkatesh Sivaraman, Katelyn Morrison, Will Epperson, Adam Perer We discuss how evaluations of human-AI decision support systems can move beyond reliance as the primary metric. Abstract As AI-based clinical decision support (AI-CDS) is introduced in more and more aspects of healthcare services, HCI research plays an increasingly important role in designing for complementarity between AI and clinicians. However, current evaluations of AI-CDS often fail to capture when AI is and is not useful to clinicians. This position paper reflects on our work and influential AI-CDS literature to advocate for moving beyond evaluation metrics like Trust, Reliance, Acceptance, and Performance on the AI’s task (what we term the “trap” of human-AI collaboration). Although these metrics can be meaningful in some simple scenarios, we argue that optimizing for them ignores important ways that AI falls short of clinical benefit, as well as ways that clinicians successfully use AI. As the fields of HCI and AI in healthcare develop new ways to design and evaluate CDS tools, we call on the community to prioritize ecologically valid, domain-appropriate study setups that measure the emergent forms of value that AI can bring to healthcare professionals. Citation Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support Venkatesh Sivaraman, Katelyn Morrison, Will Epperson, Adam Perer We discuss how evaluations of human-AI decision support systems can move beyond reliance as the primary metric. 2025 CHI Workshop on HCI+Health (CHI). Yokohama, Japan, 2025. Project PDF</summary></entry><entry><title type="html">Texture</title><link href="http://localhost:4000/papers/texture" rel="alternate" type="text/html" title="Texture" /><published>2025-05-13T00:00:00-04:00</published><updated>2025-05-13T00:00:00-04:00</updated><id>http://localhost:4000/papers/texture</id><content type="html" xml:base="http://localhost:4000/papers/texture">&lt;h1 id=&quot;texture-structured-exploration-of-text-datasets&quot;&gt;Texture: Structured Exploration of Text Datasets&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.arpit-mathur.com/&quot;&gt;Arpit Mathur&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/25-texture.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      Texture is a general purpose text visualization and exploration tool with interactions for using LLMs to derive data from text.

    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Exploratory analysis of a text corpus is essential for assessing data quality and developing meaningful hypotheses. Text analysis relies on understanding documents through structured attributes spanning various granularities of the documents such as words, phrases, sentences, topics, or clusters. However, current text visualization tools typically adopt a fixed representation tailored to specific tasks or domains, requiring users to switch tools as their analytical goals change. To address this limitation, we present Texture, a general-purpose interactive text exploration tool. Texture introduces a configurable data schema for representing text documents enriched with descriptive attributes. These attributes can appear at arbitrary levels of granularity in the text and possibly have multiple values, including document-level attributes, multi-valued attributes (e.g., topics), fine-grained span-level attributes (e.g., words), and vector embeddings. The system then combines existing interactive methods for text exploration into a single interface that provides attribute overview visualizations, supports cross-filtering attribute charts to explore subsets, uses embeddings for a dataset overview and similar instance search, and contextualizes filters in the actual documents. We evaluated Texture through a two-part user study with 10 participants from varied domains who each analyzed their own dataset in a baseline session and then with Texture. Texture was able to represent all of the previously derived dataset attributes, enabled participants to more quickly iterate during their exploratory analysis, and discover new insights about their data. Our findings contribute to the design of scalable, interactive, and flexible exploration systems that improve users’ ability to make sense of text data.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Texture: Structured Exploration of Text Datasets&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.arpit-mathur.com/&quot;&gt;Arpit Mathur&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Texture is a general purpose text exploration tool with interactions for using LLMs to derive data from text.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;Open source.  2025.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/texture&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;https://arxiv.org/abs/2504.16898&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	
	  &lt;a href=&quot;https://github.com/cmudig/Texture&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	

	

	
	
	&lt;/span&gt;




&lt;/p&gt;

&lt;!-- ## BibTeX
```


``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Texture: Structured Exploration of Text Datasets Will Epperson, Arpit Mathur, Dominik Moritz, Adam Perer Texture is a general purpose text visualization and exploration tool with interactions for using LLMs to derive data from text. Abstract Exploratory analysis of a text corpus is essential for assessing data quality and developing meaningful hypotheses. Text analysis relies on understanding documents through structured attributes spanning various granularities of the documents such as words, phrases, sentences, topics, or clusters. However, current text visualization tools typically adopt a fixed representation tailored to specific tasks or domains, requiring users to switch tools as their analytical goals change. To address this limitation, we present Texture, a general-purpose interactive text exploration tool. Texture introduces a configurable data schema for representing text documents enriched with descriptive attributes. These attributes can appear at arbitrary levels of granularity in the text and possibly have multiple values, including document-level attributes, multi-valued attributes (e.g., topics), fine-grained span-level attributes (e.g., words), and vector embeddings. The system then combines existing interactive methods for text exploration into a single interface that provides attribute overview visualizations, supports cross-filtering attribute charts to explore subsets, uses embeddings for a dataset overview and similar instance search, and contextualizes filters in the actual documents. We evaluated Texture through a two-part user study with 10 participants from varied domains who each analyzed their own dataset in a baseline session and then with Texture. Texture was able to represent all of the previously derived dataset attributes, enabled participants to more quickly iterate during their exploratory analysis, and discover new insights about their data. Our findings contribute to the design of scalable, interactive, and flexible exploration systems that improve users’ ability to make sense of text data. Citation Texture: Structured Exploration of Text Datasets Will Epperson, Arpit Mathur, Dominik Moritz, Adam Perer Texture is a general purpose text exploration tool with interactions for using LLMs to derive data from text. Open source. 2025. Project PDF Code</summary></entry><entry><title type="html">Agdebugger</title><link href="http://localhost:4000/papers/agdebugger" rel="alternate" type="text/html" title="Agdebugger" /><published>2025-03-03T00:00:00-05:00</published><updated>2025-03-03T00:00:00-05:00</updated><id>http://localhost:4000/papers/agdebugger</id><content type="html" xml:base="http://localhost:4000/papers/agdebugger">&lt;h1 id=&quot;interactive-debugging-and-steering-of-multi-agent-ai-systems&quot;&gt;Interactive Debugging and Steering of Multi-Agent AI Systems&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://gagb.github.io/&quot;&gt;Gagan Bansal&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://victordibia.com/&quot;&gt;Victor Dibia&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.adamfourney.com/&quot;&gt;Adam Fourney&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/jagerrit/&quot;&gt;Jack Gerrits&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://ekzhu.com/&quot;&gt;Erkang Zhu&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/samershi/&quot;&gt;Saleema Amershi&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/25-agdebugger.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      AGDebugger is an interactive debugging tool for multi-agent AI systems.

    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Fully autonomous teams of LLM-powered AI agents are emerging that collaborate to perform complex tasks for users. What challenges do developers face when trying to build and debug these AI agent teams? In formative interviews with five AI agent developers, we identify core challenges: difficulty reviewing long agent conversations to localize errors, lack of support in current tools for interactive debugging, and the need for tool support to iterate on agent configuration. Based on these needs, we developed an interactive multi-agent debugging tool, AGDebugger, with a UI for browsing and sending messages, the ability to edit and reset prior agent messages, and an overview visualization for navigating complex message histories. In a two-part user study with 14 participants, we identify common user strategies for steering agents and highlight the importance of interactive message resets for debugging. Our studies deepen understanding of interfaces for debugging increasingly important agentic workflows.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Interactive Debugging and Steering of Multi-Agent AI Systems&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://gagb.github.io/&quot;&gt;Gagan Bansal&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://victordibia.com/&quot;&gt;Victor Dibia&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.adamfourney.com/&quot;&gt;Adam Fourney&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/jagerrit/&quot;&gt;Jack Gerrits&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://ekzhu.com/&quot;&gt;Erkang Zhu&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/samershi/&quot;&gt;Saleema Amershi&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	AGDebugger is an interactive debugging tool for multi-agent AI systems.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;2025 CHI Conference on Human Factors in Computing Systems (CHI). Yokohama, Japan, 2025.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/agdebugger&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/agdebugger-chi25.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	
	  &lt;a href=&quot;https://github.com/microsoft/agdebugger&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	

	

	
	
	&lt;/span&gt;




&lt;/p&gt;

&lt;!-- ## BibTeX
```


``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Interactive Debugging and Steering of Multi-Agent AI Systems Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, Saleema Amershi AGDebugger is an interactive debugging tool for multi-agent AI systems. Abstract Fully autonomous teams of LLM-powered AI agents are emerging that collaborate to perform complex tasks for users. What challenges do developers face when trying to build and debug these AI agent teams? In formative interviews with five AI agent developers, we identify core challenges: difficulty reviewing long agent conversations to localize errors, lack of support in current tools for interactive debugging, and the need for tool support to iterate on agent configuration. Based on these needs, we developed an interactive multi-agent debugging tool, AGDebugger, with a UI for browsing and sending messages, the ability to edit and reset prior agent messages, and an overview visualization for navigating complex message histories. In a two-part user study with 14 participants, we identify common user strategies for steering agents and highlight the importance of interactive message resets for debugging. Our studies deepen understanding of interfaces for debugging increasingly important agentic workflows. Citation Interactive Debugging and Steering of Multi-Agent AI Systems Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, Saleema Amershi AGDebugger is an interactive debugging tool for multi-agent AI systems. 2025 CHI Conference on Human Factors in Computing Systems (CHI). Yokohama, Japan, 2025. Project PDF Code</summary></entry><entry><title type="html">Guidedstats</title><link href="http://localhost:4000/papers/guidedstats" rel="alternate" type="text/html" title="Guidedstats" /><published>2024-10-15T00:00:00-04:00</published><updated>2024-10-15T00:00:00-04:00</updated><id>http://localhost:4000/papers/guidedstats</id><content type="html" xml:base="http://localhost:4000/papers/guidedstats">&lt;h1 id=&quot;guided-statistical-workflows-with-interactive-explanations-and-assumption-checking&quot;&gt;Guided Statistical Workflows with Interactive Explanations and Assumption Checking&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;https://www.linkedin.com/in/yuqi-zhang-a3541a1a0/&quot;&gt;Yuqi Zhang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/24-guidedstats.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      GuidedStats is a Jupyter extension that helps data scientists perform statistical analyses with guided workflows.

    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Statistical practices such as building regression models or running hypothesis tests rely on following rigorous procedures of steps and verifying assumptions on data to produce valid results. However, common statistical tools do not verify users’ decision choices and provide low-level statistical functions without instructions on the whole analysis practice. Users can easily misuse analysis methods, potentially decreasing the validity of results. To address this problem, we introduce GuidedStats, an interactive interface within computational notebooks that encapsulates guidance, models, visualization, and exportable results into interactive workflows. It breaks down typical analysis processes, such as linear regression and two-sample T-tests, into interactive steps supplemented with automatic visualizations and explanations for step-wise evaluation. Users can iterate on input choices to refine their models, while recommended actions and exports allow the user to continue their analysis in code. Case studies show how GuidedStats offers valuable instructions for conducting fluid statistical analyses while finding possible assumption violations in the underlying data, supporting flexible and accurate statistical analyses.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Guided Statistical Workflows with Interactive Explanations and Assumption Checking&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;https://www.linkedin.com/in/yuqi-zhang-a3541a1a0/&quot;&gt;Yuqi Zhang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	GuidedStats is a Jupyter extension that helps data scientists perform statistical analyses with guided workflows.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;VIS 24: IEEE Conference on Data Visualization (VIS). St Pete Beach, Florida, 2024.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/guidedstats&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/guidedstats_vis24.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	
	  &lt;a href=&quot;https://github.com/cmudig/GuidedStats&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	

	

	
	
	&lt;/span&gt;




&lt;/p&gt;

&lt;!-- ## BibTeX
```


``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Guided Statistical Workflows with Interactive Explanations and Assumption Checking Yuqi Zhang, Adam Perer, Will Epperson GuidedStats is a Jupyter extension that helps data scientists perform statistical analyses with guided workflows. Abstract Statistical practices such as building regression models or running hypothesis tests rely on following rigorous procedures of steps and verifying assumptions on data to produce valid results. However, common statistical tools do not verify users’ decision choices and provide low-level statistical functions without instructions on the whole analysis practice. Users can easily misuse analysis methods, potentially decreasing the validity of results. To address this problem, we introduce GuidedStats, an interactive interface within computational notebooks that encapsulates guidance, models, visualization, and exportable results into interactive workflows. It breaks down typical analysis processes, such as linear regression and two-sample T-tests, into interactive steps supplemented with automatic visualizations and explanations for step-wise evaluation. Users can iterate on input choices to refine their models, while recommended actions and exports allow the user to continue their analysis in code. Case studies show how GuidedStats offers valuable instructions for conducting fluid statistical analyses while finding possible assumption violations in the underlying data, supporting flexible and accurate statistical analyses. Citation Guided Statistical Workflows with Interactive Explanations and Assumption Checking Yuqi Zhang, Adam Perer, Will Epperson GuidedStats is a Jupyter extension that helps data scientists perform statistical analyses with guided workflows. VIS 24: IEEE Conference on Data Visualization (VIS). St Pete Beach, Florida, 2024. Project PDF Code</summary></entry><entry><title type="html">Quickdashboard</title><link href="http://localhost:4000/papers/quickdashboard" rel="alternate" type="text/html" title="Quickdashboard" /><published>2023-09-24T00:00:00-04:00</published><updated>2023-09-24T00:00:00-04:00</updated><id>http://localhost:4000/papers/quickdashboard</id><content type="html" xml:base="http://localhost:4000/papers/quickdashboard">&lt;h1 id=&quot;a-declarative-specification-for-authoring-metrics-dashboards&quot;&gt;A Declarative Specification for Authoring Metrics Dashboards&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://kanitw.github.io/&quot;&gt;Kanit Wongsuphasawat&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.linkedin.com/in/allisonwhilden/&quot;&gt;Allison Whilden&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.linkedin.com/in/fandu/&quot;&gt;Fan Du&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.linkedin.com/in/justinftalbot/&quot;&gt;Justin Talbot&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/23-quickdashboard-vis.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      Quick dashboarding presents a novel specification for dashboard authoring, comprised of sections of metrics combined with dimensions.

    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Despite their ubiquity, authoring dashboards for metrics reporting in modern data analysis tools remains a manual, time-consuming process. 
Rather than focusing on interesting combinations of their data, users have to spend time creating each chart in a dashboard one by one. 
This makes dashboard creation slow and tedious. We conducted a review of production metrics dashboards and found that many dashboards contain a 
common structure: breaking down one or more metrics by different dimensions. In response, we developed a high-level specification for describing 
dashboards as sections of metrics repeated across the same dimensions and a graphical interface, Quick Dashboard, for authoring dashboards based on 
this specification. We present several usage examples that demonstrate the flexibility of this specification to create various kinds of dashboards 
and support a data-first approach to dashboard authoring.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;A Declarative Specification for Authoring Metrics Dashboards&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://kanitw.github.io/&quot;&gt;Kanit Wongsuphasawat&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.linkedin.com/in/allisonwhilden/&quot;&gt;Allison Whilden&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.linkedin.com/in/fandu/&quot;&gt;Fan Du&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.linkedin.com/in/justinftalbot/&quot;&gt;Justin Talbot&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Quick dashboarding presents a novel specification for dashboard authoring, comprised of sections of metrics combined with dimensions.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;VDS at VIS 23: Visual Data Science Symposium (VDS). Melbourne, Australia, 2023.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/quickdashboard&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/quickdashboard-vds23.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	

	

	

	

	
	    &lt;span class=&quot;cv-award&quot;&gt;&lt;i class=&quot;fas fa-trophy&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Best Paper&lt;/span&gt;
	

	
	
	&lt;/span&gt;




&lt;/p&gt;

&lt;!-- ## BibTeX
```


``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">A Declarative Specification for Authoring Metrics Dashboards Will Epperson, Kanit Wongsuphasawat, Allison Whilden, Fan Du, Justin Talbot Quick dashboarding presents a novel specification for dashboard authoring, comprised of sections of metrics combined with dimensions. Abstract Despite their ubiquity, authoring dashboards for metrics reporting in modern data analysis tools remains a manual, time-consuming process. Rather than focusing on interesting combinations of their data, users have to spend time creating each chart in a dashboard one by one. This makes dashboard creation slow and tedious. We conducted a review of production metrics dashboards and found that many dashboards contain a common structure: breaking down one or more metrics by different dimensions. In response, we developed a high-level specification for describing dashboards as sections of metrics repeated across the same dimensions and a graphical interface, Quick Dashboard, for authoring dashboards based on this specification. We present several usage examples that demonstrate the flexibility of this specification to create various kinds of dashboards and support a data-first approach to dashboard authoring. Citation A Declarative Specification for Authoring Metrics Dashboards Will Epperson, Kanit Wongsuphasawat, Allison Whilden, Fan Du, Justin Talbot Quick dashboarding presents a novel specification for dashboard authoring, comprised of sections of metrics combined with dimensions. VDS at VIS 23: Visual Data Science Symposium (VDS). Melbourne, Australia, 2023. Project PDF Best Paper</summary></entry><entry><title type="html">Autoprofiler</title><link href="http://localhost:4000/papers/autoprofiler" rel="alternate" type="text/html" title="Autoprofiler" /><published>2023-09-24T00:00:00-04:00</published><updated>2023-09-24T00:00:00-04:00</updated><id>http://localhost:4000/papers/autoprofiler</id><content type="html" xml:base="http://localhost:4000/papers/autoprofiler">&lt;h1 id=&quot;dead-or-alive-continuous-data-profiling-for-interactive-data-science&quot;&gt;Dead or Alive: Continuous Data Profiling for Interactive Data Science&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.linkedin.com/in/vaishnavi-gorantla/&quot;&gt;Vaishnavi Gorantla&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/23-autoprofiler-vis.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      AutoProfiler is a Jupyter extension that helps data scientists understand their data and find issues during analysis through continuous data profiling.

    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is 
manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists 
profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose 
continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis 
to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays 
data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically 
as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, 
we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, 
however, one version updates reactively (“live”) and the other updates only on demand (“dead”). We find that both tools, dead or alive, facilitate insight 
discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates 
intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also 
present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data 
profiles. Our results have implications for the design of future tools that offer automated data analysis support.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Dead or Alive: Continuous Data Profiling for Interactive Data Science&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.linkedin.com/in/vaishnavi-gorantla/&quot;&gt;Vaishnavi Gorantla&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	AutoProfiler is a Jupyter extension that helps data scientists understand their data and find issues during analysis through continuous data profiling.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;VIS 23: IEEE Conference on Data Visualization (VIS). Melbourne, Australia, 2023.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/autoprofiler&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/autoprofiler-vis23.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	
	  &lt;a href=&quot;https://github.com/cmudig/AutoProfiler&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	

	
	    &lt;span class=&quot;cv-award&quot;&gt;&lt;i class=&quot;fas fa-trophy&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention&lt;/span&gt;
	

	
	
	&lt;/span&gt;




&lt;/p&gt;

&lt;!-- ## BibTeX
```


``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Dead or Alive: Continuous Data Profiling for Interactive Data Science Will Epperson, Vaishnavi Gorantla, Dominik Moritz, Adam Perer AutoProfiler is a Jupyter extension that helps data scientists understand their data and find issues during analysis through continuous data profiling. Abstract Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively (“live”) and the other updates only on demand (“dead”). We find that both tools, dead or alive, facilitate insight discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support. Citation Dead or Alive: Continuous Data Profiling for Interactive Data Science Will Epperson, Vaishnavi Gorantla, Dominik Moritz, Adam Perer AutoProfiler is a Jupyter extension that helps data scientists understand their data and find issues during analysis through continuous data profiling. VIS 23: IEEE Conference on Data Visualization (VIS). Melbourne, Australia, 2023. Project PDF Code Best Paper Honorable Mention</summary></entry><entry><title type="html">Solas</title><link href="http://localhost:4000/papers/solas" rel="alternate" type="text/html" title="Solas" /><published>2022-04-24T00:00:00-04:00</published><updated>2022-04-24T00:00:00-04:00</updated><id>http://localhost:4000/papers/solas</id><content type="html" xml:base="http://localhost:4000/papers/solas">&lt;h1 id=&quot;leveraging-analysis-history-for-improved-in-situ-visualization-recommendation&quot;&gt;Leveraging Analysis History for Improved In Situ Visualization Recommendation&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://dorisjunglinlee.com&quot;&gt;Doris Jung-Lin Lee&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://leijiewang.com&quot;&gt;Leijie Wang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.kunal-agarwal.com&quot;&gt;Kunal Agarwal&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://people.eecs.berkeley.edu/~adityagp/&quot;&gt;Aditya Parameswaran&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/22-solas-eurovis.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      Solas tracks the history of a user’s analysis to provide improved in situ visualization recommendations. Above, a user has most
recently created the Class column that is visualized on the left side of the interface. Recently executed Pandas commands interacted with
Worldwide_Gross, Viewership, and MPAA_Rating; therefore, Class is shown relative to these columns.


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Existing visualization recommendation systems commonly rely on a single snapshot of a dataset to suggest visualizations to users. 
However, exploratory data analysis involves a series of related interactions with a dataset over time rather than one-off analytical steps. 
We present Solas, a tool that tracks the history of a user’s data analysis, models their interest in each column, and 
uses this information to provide visualization recommendations, all within the user’s native analytical environment. 
Recommending with analysis history improves visualizations in three primary ways: task-specific visualizations use the provenance of 
data to provide sensible encodings for common analysis functions, aggregated history is used to rank visualizations by our model of a 
user’s interest in each column, and column data types are inferred based on applied operations. We present a usage scenario and a user evaluation 
demonstrating how leveraging analysis history improves in situ visualization recommendations on real-world analysis tasks.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Leveraging Analysis History for Improved In Situ Visualization Recommendation&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://dorisjunglinlee.com&quot;&gt;Doris Jung-Lin Lee&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://leijiewang.com&quot;&gt;Leijie Wang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.kunal-agarwal.com&quot;&gt;Kunal Agarwal&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://people.eecs.berkeley.edu/~adityagp/&quot;&gt;Aditya Parameswaran&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.domoritz.de/&quot;&gt;Dominik Moritz&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://perer.org&quot;&gt;Adam Perer&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Solas is a visualization recommendation tool that uses the history of analysis for in situ recommendations in Jupyter.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;EuroVis 22: Eurographics Conference on Visualization (EuroVis). Rome, Italy, 2022.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/solas&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/Solas_EuroVis22.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	
	  &lt;a href=&quot;https://github.com/cmudig/solas&quot;&gt;
	    &lt;i class=&quot;fas fa-code&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Code
	  &lt;/a&gt;
	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{Epperson22Solas,
  title={Leveraging Analysis History for Improved In Situ Visualization Recommendation},
  author={Epperson, Will and Lee, Doris Jung-Lin and Wang, Leijie and Agarwal, Kunal and Parameswaran, Aditya and Moritz, Dominik and Perer, Adam},
  journal={EuroVis 22: Eurographics Conference on Visualization},
  year={2022},
  publisher={EG}
  url={}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Leveraging Analysis History for Improved In Situ Visualization Recommendation Will Epperson, Doris Jung-Lin Lee, Leijie Wang, Kunal Agarwal, Aditya Parameswaran, Dominik Moritz, Adam Perer Solas tracks the history of a user’s analysis to provide improved in situ visualization recommendations. Above, a user has most recently created the Class column that is visualized on the left side of the interface. Recently executed Pandas commands interacted with Worldwide_Gross, Viewership, and MPAA_Rating; therefore, Class is shown relative to these columns. Abstract Existing visualization recommendation systems commonly rely on a single snapshot of a dataset to suggest visualizations to users. However, exploratory data analysis involves a series of related interactions with a dataset over time rather than one-off analytical steps. We present Solas, a tool that tracks the history of a user’s data analysis, models their interest in each column, and uses this information to provide visualization recommendations, all within the user’s native analytical environment. Recommending with analysis history improves visualizations in three primary ways: task-specific visualizations use the provenance of data to provide sensible encodings for common analysis functions, aggregated history is used to rank visualizations by our model of a user’s interest in each column, and column data types are inferred based on applied operations. We present a usage scenario and a user evaluation demonstrating how leveraging analysis history improves in situ visualization recommendations on real-world analysis tasks. Citation Leveraging Analysis History for Improved In Situ Visualization Recommendation Will Epperson, Doris Jung-Lin Lee, Leijie Wang, Kunal Agarwal, Aditya Parameswaran, Dominik Moritz, Adam Perer Solas is a visualization recommendation tool that uses the history of analysis for in situ recommendations in Jupyter. EuroVis 22: Eurographics Conference on Visualization (EuroVis). Rome, Italy, 2022. Project PDF Code</summary></entry><entry><title type="html">Ditl</title><link href="http://localhost:4000/papers/ditl" rel="alternate" type="text/html" title="Ditl" /><published>2022-02-16T00:00:00-05:00</published><updated>2022-02-16T00:00:00-05:00</updated><id>http://localhost:4000/papers/ditl</id><content type="html" xml:base="http://localhost:4000/papers/ditl">&lt;h1 id=&quot;diff-in-the-loop-supporting-data-comparison-in-exploratory-data-analysis&quot;&gt;Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/22-ditl-chi.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      As users iterate on their data during analysis, they can use DITL to compare data snapshots. 
Every time users successfully execute code we save a snapshot (A). 
Users can compare the code using traditional code diffing tools. 
Additionally, users can also use DITL to compare data iterations with interactive visualizations, descriptive statistics, and data preview (B). 
Users can choose three ways to visualize the differences in each column: the delta view (C), opacity view (D), and parallel view (E).


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Data science is characterized by evolution: since data science is exploratory, results evolve from moment to moment; since it can be collaborative, results evolve as the work changes hands. 
While existing tools help data scientists track changes in code, they provide less support for understanding the iterative changes that the code produces in the data. 
We explore the idea of visualizing differences in datasets as a core feature of exploratory data analysis, a concept we call Diff in the Loop (DITL). 
We evaluated DITL in a user study with 16 professional data scientists and found it helped them understand the implications of their actions when manipulating data. 
We summarize these findings and discuss how the approach can be generalized to different data science workflows.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Diff in the Loop supports tracking, comparing, and visualizing differences in datasets during iterative data analysis.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;SIGCHI 22: ACM Symposium on Computer Human Interaction (CHI). New Orleans, LA, 2022.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/ditl&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/ditl-chi22.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{wang2022DITL,
  title={Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis},
  author={Wang, April Yi and Epperson, Will, and DeLine, Robert and Drucker, Steven M.},
  journal={SIGCHI 22: ACM Symposium on Computer Human Interaction},
  year={2022},
  publisher={ACM}
  url={}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis April Yi Wang, Will Epperson, Robert DeLine, Steven M. Drucker As users iterate on their data during analysis, they can use DITL to compare data snapshots. Every time users successfully execute code we save a snapshot (A). Users can compare the code using traditional code diffing tools. Additionally, users can also use DITL to compare data iterations with interactive visualizations, descriptive statistics, and data preview (B). Users can choose three ways to visualize the differences in each column: the delta view (C), opacity view (D), and parallel view (E). Abstract Data science is characterized by evolution: since data science is exploratory, results evolve from moment to moment; since it can be collaborative, results evolve as the work changes hands. While existing tools help data scientists track changes in code, they provide less support for understanding the iterative changes that the code produces in the data. We explore the idea of visualizing differences in datasets as a core feature of exploratory data analysis, a concept we call Diff in the Loop (DITL). We evaluated DITL in a user study with 16 professional data scientists and found it helped them understand the implications of their actions when manipulating data. We summarize these findings and discuss how the approach can be generalized to different data science workflows. Citation Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis April Yi Wang, Will Epperson, Robert DeLine, Steven M. Drucker Diff in the Loop supports tracking, comparing, and visualizing differences in datasets during iterative data analysis. SIGCHI 22: ACM Symposium on Computer Human Interaction (CHI). New Orleans, LA, 2022. Project PDF</summary></entry><entry><title type="html">Reuseshareds</title><link href="http://localhost:4000/papers/reuse-ds" rel="alternate" type="text/html" title="Reuseshareds" /><published>2022-02-16T00:00:00-05:00</published><updated>2022-02-16T00:00:00-05:00</updated><id>http://localhost:4000/papers/reuseshareds</id><content type="html" xml:base="http://localhost:4000/papers/reuse-ds">&lt;h1 id=&quot;strategies-for-reuse-and-sharing-among-data-scientists-in-software-teams&quot;&gt;Strategies for Reuse and Sharing among Data Scientists in Software Teams&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/22-reuse-share-DS-icse.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      Five unique strategies are used to reuse and share analysis code in data science. Personal reuse strategies like reusing one&apos;s own code are common, whereas using template notebooks is more rare and dependent on tool support.


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Effective sharing and reuse practices have long been hallmarks of proficient software engineering. 
Yet the exploratory nature of data science presents new challenges and opportunities to support sharing and reuse of analysis code. 
To better understand current practices, we conducted interviews (N=17) and a survey (N=132) with data scientists at Microsoft, and extract five commonly used strategies for sharing and reuse of past work: 
personal analysis reuse, personal utility libraries, team shared analysis code, team shared template notebooks, and team shared libraries. 
We also identify factors that encourage or discourage data scientists from sharing and reusing. 
Our participants described obstacles to reuse and sharing including a lack of incentives to create shared code, difficulties in making data science code modular, and a lack of tool interoperability. 
We discuss how future tools might help meet these needs.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;Strategies for Reuse and Sharing among Data Scientists in Software Teams&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://aprilwang.me&quot;&gt;April Yi Wang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/rdeline/&quot;&gt;Robert DeLine&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/sdrucker/&quot;&gt;Steven M. Drucker&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Interviews and a survey with 149 data scientists at Microsoft revealed five distinct strategies for sharing and reusing analysis code along with factors that encourage or discourage reuse.
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;ICSE 22: ACM International Conference on Software Engineering (ICSE). Pittsburgh, PA, 2022.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/reuse-ds&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;/papers/reuse-sharing-DS-icse22.pdf&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	
	&lt;a href=&quot;https://www.youtube.com/watch?v=W4XAF2vkoCQ&quot;&gt;
		&lt;i class=&quot;fas fa-video&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Recording
	&lt;/a&gt;
	

	
	  &lt;a href=&quot;/slides/ICSE22_Reuse_Sharing_Slides.pdf&quot;&gt;
	    &lt;i class=&quot;fas fa-window-maximize&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Slides
	  &lt;/a&gt;
	

	

	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{wang2022DITL,
  title={Strategies for Reuse and Sharing among Data Scientists in Software Teams},
  author={Epperson, Will and Wang, April Yi and DeLine, Robert and Drucker, Steven M.},
  journal={ICSE 22: ACM International Conference on Software Engineering},
  year={2022},
  publisher={ACM}
  url={}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">Strategies for Reuse and Sharing among Data Scientists in Software Teams Will Epperson, April Yi Wang, Robert DeLine, Steven M. Drucker Five unique strategies are used to reuse and share analysis code in data science. Personal reuse strategies like reusing one&apos;s own code are common, whereas using template notebooks is more rare and dependent on tool support. Abstract Effective sharing and reuse practices have long been hallmarks of proficient software engineering. Yet the exploratory nature of data science presents new challenges and opportunities to support sharing and reuse of analysis code. To better understand current practices, we conducted interviews (N=17) and a survey (N=132) with data scientists at Microsoft, and extract five commonly used strategies for sharing and reuse of past work: personal analysis reuse, personal utility libraries, team shared analysis code, team shared template notebooks, and team shared libraries. We also identify factors that encourage or discourage data scientists from sharing and reusing. Our participants described obstacles to reuse and sharing including a lack of incentives to create shared code, difficulties in making data science code modular, and a lack of tool interoperability. We discuss how future tools might help meet these needs. Citation Strategies for Reuse and Sharing among Data Scientists in Software Teams Will Epperson, April Yi Wang, Robert DeLine, Steven M. Drucker Interviews and a survey with 149 data scientists at Microsoft revealed five distinct strategies for sharing and reusing analysis code along with factors that encourage or discourage reuse. ICSE 22: ACM International Conference on Software Engineering (ICSE). Pittsburgh, PA, 2022. Project PDF Recording Slides</summary></entry><entry><title type="html">Recast</title><link href="http://localhost:4000/papers/recast" rel="alternate" type="text/html" title="Recast" /><published>2020-01-08T00:00:00-05:00</published><updated>2020-01-08T00:00:00-05:00</updated><id>http://localhost:4000/papers/recast</id><content type="html" xml:base="http://localhost:4000/papers/recast">&lt;h1 id=&quot;recast-interactive-auditing-of-automatic-toxicity-detection-models&quot;&gt;RECAST: Interactive Auditing of Automatic Toxicity Detection Models&lt;/h1&gt;
&lt;p&gt;

  
    
&lt;a href=&quot;http://austinpwright.com&quot;&gt;Austin P. Wright&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://oshaikh.com/&quot;&gt;Omar Shaikh&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://haekyu.com&quot;&gt;Haekyu Park&lt;/a&gt;,
  

  
    
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
  

  
    
Muhammed Ahmed,
  

  
    
Stephane Pinel,
  

  
    
&lt;a href=&quot;https://www.cc.gatech.edu/~dyang888/&quot;&gt;Diyi Yang&lt;/a&gt;,
  

  
    
&lt;a href=&quot;https://www.cc.gatech.edu/~dchau&quot;&gt;Duen Horng (Polo) Chau&lt;/a&gt;
  

&lt;/p&gt;

&lt;figure&gt;
    &lt;img class=&quot;single&quot; src=&quot;/images/papers/20-recast-chi.png&quot; /&gt;
    &lt;figcaption class=&quot;single&quot;&gt;
      A: RECAST consists of a textbox and a radial progress bar. A color change on the radial progress, along with a score, indicate the toxicity of a sentence. 
Toxicity ranges from white (non-toxic) to red (very toxic). Users can hover over options to preview toxicity scores for replacing the selected word in the sentence. 
B: upon replacing the word (in the case of this figure, replacing “idiotic” with “nonsensical”), the main radial progress bar reflects the reduced toxicity score. 
However the small attention on the other pejorative word &quot;moron&quot; compared to &quot;video&quot; in the alternative version shows the idiosyncrasies of the model and underlying dataset.


    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;As toxic language becomes nearly pervasive online, there has been increasing interest in leveraging the advancements in natural language processing (NLP) to automatically detect and remove toxic comments. 
Despite fairness concerns and limited interpretability, there is currently little work for auditing these systems in particular for end users. 
We present our ongoing work, RECAST, an interactive tool for auditing toxicity detection models by visualizing explanations for predictions and providing alternative wordings for detected toxic speech. 
RECAST displays the attention of toxicity detection models on user input, and provides an intuitive system for rewording impactful language within a comment with less toxic alternative words close in embedding space. 
Finally we propose a larger user study  of RECAST, with promising preliminary results, to validate it’s effectiveness and useability with end users.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;
&lt;p&gt;

	&lt;strong&gt;RECAST: Interactive Auditing of Automatic Toxicity Detection Models&lt;/strong&gt;


&lt;br /&gt;


	
		
&lt;a href=&quot;http://austinpwright.com&quot;&gt;Austin P. Wright&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://oshaikh.com/&quot;&gt;Omar Shaikh&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://haekyu.com&quot;&gt;Haekyu Park&lt;/a&gt;,
	

	
		
&lt;a href=&quot;http://www.willepperson.com&quot;&gt;Will Epperson&lt;/a&gt;,
	

	
		
Muhammed Ahmed,
	

	
		
Stephane Pinel,
	

	
		
&lt;a href=&quot;https://www.cc.gatech.edu/~dyang888/&quot;&gt;Diyi Yang&lt;/a&gt;,
	

	
		
&lt;a href=&quot;https://www.cc.gatech.edu/~dchau&quot;&gt;Duen Horng (Polo) Chau&lt;/a&gt;
	


&lt;br /&gt;


&lt;span class=&quot;cv-description&quot;&gt;
	Interactive Auditing of Automatic Toxicity Detection Models
&lt;/span&gt;
&lt;br /&gt;


&lt;i&gt;24th ACM Conference on Computer-Supported Cooperative Work &amp;amp; Social Computing.  2021.&lt;/i&gt;

&lt;br /&gt;


	&lt;span class=&quot;pub-misc&quot;&gt;

	

	
	  &lt;a href=&quot;/papers/recast&quot;&gt;
	    &lt;i class=&quot;fas fa-link&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Project
	  &lt;/a&gt;
	

	

	
	  &lt;a href=&quot;https://arxiv.org/pdf/2102.04427&quot;&gt;
	    &lt;i class=&quot;far fa-file-pdf&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; PDF
	  &lt;/a&gt;
	

	

	

	

	

	

	

	

	

	

	

	
		
		
	

	

	
	
	&lt;/span&gt;







&lt;/p&gt;

&lt;!-- ## BibTeX
```
@article{wright2020recast,
title={RECAST: Interactive Auditing of Automatic Toxicity Detection Models},
author={Austin P. Wright and Omar Shaikh and Haekyu Park and Will Epperson and Muhammed Ahmed and Stephane Pinel and Diyi Yang and Duen Horng (Polo) Chau},
year={2020},
eprint={2001.01819},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

``` --&gt;</content><author><name>Will Epperson</name><email>willepp@cmu.edu</email></author><summary type="html">RECAST: Interactive Auditing of Automatic Toxicity Detection Models Austin P. Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed, Stephane Pinel, Diyi Yang, Duen Horng (Polo) Chau A: RECAST consists of a textbox and a radial progress bar. A color change on the radial progress, along with a score, indicate the toxicity of a sentence. Toxicity ranges from white (non-toxic) to red (very toxic). Users can hover over options to preview toxicity scores for replacing the selected word in the sentence. B: upon replacing the word (in the case of this figure, replacing “idiotic” with “nonsensical”), the main radial progress bar reflects the reduced toxicity score. However the small attention on the other pejorative word &quot;moron&quot; compared to &quot;video&quot; in the alternative version shows the idiosyncrasies of the model and underlying dataset. Abstract As toxic language becomes nearly pervasive online, there has been increasing interest in leveraging the advancements in natural language processing (NLP) to automatically detect and remove toxic comments. Despite fairness concerns and limited interpretability, there is currently little work for auditing these systems in particular for end users. We present our ongoing work, RECAST, an interactive tool for auditing toxicity detection models by visualizing explanations for predictions and providing alternative wordings for detected toxic speech. RECAST displays the attention of toxicity detection models on user input, and provides an intuitive system for rewording impactful language within a comment with less toxic alternative words close in embedding space. Finally we propose a larger user study of RECAST, with promising preliminary results, to validate it’s effectiveness and useability with end users. Citation RECAST: Interactive Auditing of Automatic Toxicity Detection Models Austin P. Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed Ahmed, Stephane Pinel, Diyi Yang, Duen Horng (Polo) Chau Interactive Auditing of Automatic Toxicity Detection Models 24th ACM Conference on Computer-Supported Cooperative Work &amp;amp; Social Computing. 2021. Project PDF</summary></entry></feed>